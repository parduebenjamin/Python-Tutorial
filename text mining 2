import pandas as pd
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop = stopwords.words('english')

train = pd.read_csv('\\\\RTNAS01\\data\\RT-Claims-SIU-User\\Renton_Analyst_Files\\ACA Files\\1 ACAs in Progress\\Check Washing Predictive Analytics\\CHKW_claims_forText_Mining.csv')

train['word_count'] = train['BODYTEXT'].apply(lambda x: len(str(x).split(" ")))
train['char_count'] = train['BODYTEXT'].str.len() ## this also includes spaces

def avg_word(sentence):
  words = sentence.split()
  return (sum(len(word) for word in words)/len(words))
train['avg_word'] = train['BODYTEXT'].apply(lambda x: avg_word(x))

train['stopwords'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x in stop]))
train['hastags'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))
train['numerics'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
train['upper'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x.isupper()]))

train[['BODYTEXT','word_count','char_count','avg_word','stopwords','hastags','numerics','upper']].head()

#break------

import pandas as pd
import nltk
nltk.download('wordnet')
import textblob
from nltk.corpus import stopwords
from textblob import TextBlob
from nltk.stem import PorterStemmer
from textblob import Word



stop = stopwords.words('english')
train = pd.read_csv('\\\\RTNAS01\\data\\RT-Claims-SIU-User\\Renton_Analyst_Files\\ACA Files\\1 ACAs in Progress\\Check Washing Predictive Analytics\\CHKW_claims_forText_Mining.csv')
st = PorterStemmer()

train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join(x.lower() for x in x.split()))
train['BODYTEXT'] = train['BODYTEXT'].str.replace('[^\w\s]','')
train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
freq = pd.Series(' '.join(train['BODYTEXT']).split()).value_counts()[:10]
unfreq = pd.Series(' '.join(train['BODYTEXT']).split()).value_counts()[-10:]
freq = list(freq.index)
train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
unfreq = list(unfreq.index)
#train['tweet'] = train['tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in unfreq))
#train['tweet'][:5].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))
#this is for stemming, but we usually prefer lemmatization
train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
train['BODYTEXT'].head()





#unfreq
#freq
#print freq to see the most common words

#break------
import pandas as pd
import nltk
nltk.download('wordnet')
nltk.download('stopwords')

import textblob
from nltk.corpus import stopwords
from textblob import TextBlob
from nltk.stem import PorterStemmer
from textblob import Word

from nltk.corpus import stopwords
stop = stopwords.words('english')

stop = stopwords.words('english')
train = pd.read_csv('\\\\RTNAS01\\data\\RT-Claims-SIU-User\\Renton_Analyst_Files\\ACA Files\\1 ACAs in Progress\\Check Washing Predictive Analytics\\CHKW_claims_forText_Mining.csv')
st = PorterStemmer()

train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join(x.lower() for x in x.split()))
train['BODYTEXT'] = train['BODYTEXT'].str.replace('[^\w\s]','')
train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
freq = pd.Series(' '.join(train['BODYTEXT']).split()).value_counts()[:10]
unfreq = pd.Series(' '.join(train['BODYTEXT']).split()).value_counts()[-10:]
freq = list(freq.index)
train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
unfreq = list(unfreq.index)
train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join(x for x in x.split() if x not in unfreq))
train['BODYTEXT'][:5].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))
#this is for stemming, but we usually prefer lemmatization
train['BODYTEXT'] = train['BODYTEXT'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
#train['BODYTEXT'].head()

train['word_count'] = train['BODYTEXT'].apply(lambda x: len(str(x).split(" ")))
train['char_count'] = train['BODYTEXT'].str.len() ## this also includes spaces

def avg_word(sentence):
  words = sentence.split()
  return (sum(len(word) for word in words)/len(words))
train['avg_word'] = train['BODYTEXT'].apply(lambda x: avg_word(x))

train['stopwords'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x in stop]))
train['hastags'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))
train['numerics'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))
train['upper'] = train['BODYTEXT'].apply(lambda x: len([x for x in x.split() if x.isupper()]))

train[['BODYTEXT','word_count','char_count','avg_word','stopwords','hastags','numerics','upper']].head()

#unfreq
#freq
#print freq to see the most common words
